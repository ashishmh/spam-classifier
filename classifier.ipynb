{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, sys, time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = '../enron1/'\n",
    "HAM_FOLDER = 'ham/'\n",
    "SPAM_FOLDER = 'spam/'\n",
    "\n",
    "HAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "HAM_LIST = glob.glob(FOLDER + HAM_FOLDER + '*.txt')\n",
    "SPAM_LIST = glob.glob(FOLDER + SPAM_FOLDER + '*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def __init__(self, ham_list, spam_list):\n",
    "        self.ham_list = ham_list\n",
    "        self.spam_list = spam_list\n",
    "        self.email_list = ham_list + spam_list\n",
    "        self.N_HAM = np.size(ham_list)\n",
    "        self.N_SPAM = np.size(spam_list)\n",
    "        self.N = np.asarray([self.N_HAM, self.N_SPAM])\n",
    "        self.label = np.asarray([HAM]* self.N_HAM + [SPAM]* self.N_SPAM)\n",
    "        \n",
    "        # number of training docs in ham and spam folder\n",
    "        # default to be 80% of total\n",
    "        self.p_training = 0.8\n",
    "        self.N_TRAINING = np.asarray([int(np.floor(self.N_HAM * 0.8)), int(np.floor(self.N_SPAM * 0.8))])\n",
    "        self.N_TESTING = self.N - self.N_TRAINING\n",
    "        \n",
    "        self.training_X = None # vectorized training data\n",
    "        self.training_label = None\n",
    "        self.testing_X = None # vectorized testing data\n",
    "        self.testing_label = None\n",
    "        self.result = []\n",
    "        \n",
    "        # container for vocabulary list\n",
    "        self.vocab = []\n",
    "        self.nvocab = 0\n",
    "        \n",
    "    def nwords(self, X):\n",
    "        '''return the number of distinct words in input matrix X by counting the non-empty columns in X\n",
    "            parameters:\n",
    "                X: 2d numpy array'''\n",
    "        return np.count_nonzero(X.sum(axis = 0))\n",
    "    \n",
    "    def vectorize(self, p_training = None):\n",
    "        print('vectorizing the emails...')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if p_training != None:\n",
    "            self.p_training = p_training\n",
    "        # else p_training = 0.8 by default\n",
    "        \n",
    "        # [number of ham in training, number of spam in training]\n",
    "        self.N_TRAINING = np.asarray([int(np.floor(self.N_HAM * self.p_training)),\n",
    "                                      int(np.floor(self.N_SPAM * self.p_training))])\n",
    "        # [number of ham in testing, number of spam in testing]\n",
    "        self.N_TESTING = self.N - self.N_TRAINING\n",
    "        \n",
    "        # word stemming\n",
    "        stemmer = EnglishStemmer()\n",
    "        analyzer = CountVectorizer(input = 'filename', decode_error = 'ignore').build_analyzer()\n",
    "        def stemmed_words(doc):\n",
    "            return (stemmer.stem(w) for w in analyzer(doc))\n",
    "        \n",
    "        training = CountVectorizer(input = 'filename', decode_error = 'ignore', analyzer = stemmed_words, \n",
    "                                   max_df = 0.95, min_df = 5)\n",
    "        self.training_X = training.fit_transform(self.ham_list[:self.N_TRAINING[HAM]] + self.spam_list[:self.N_TRAINING[SPAM]]).toarray()\n",
    "        \n",
    "        # get the vocabulary list from training data\n",
    "        self.vocab = training.get_feature_names()\n",
    "        self.nvocab = np.size(self.vocab)\n",
    "\n",
    "        testing = CountVectorizer(input = 'filename', vocabulary = self.vocab, decode_error = 'ignore')\n",
    "        self.testing_X = testing.fit_transform(self.ham_list[-self.N_TESTING[HAM]:] + self.spam_list[-self.N_TESTING[SPAM]:]).toarray()\n",
    "        \n",
    "        # create the label arrays\n",
    "        self.training_label = np.asarray([HAM] * self.N_TRAINING[HAM] + [SPAM] * self.N_TRAINING[SPAM])\n",
    "        self.testing_label = np.asarray([HAM] * self.N_TESTING[HAM] + [SPAM] * self.N_TESTING[SPAM])\n",
    "        \n",
    "        print('vectorizing took %.2f s' % (time.time() - start_time))\n",
    "        return self.training_X, self.training_label, self.testing_X, self.testing_label\n",
    "    \n",
    "    def get_ham(self, X, label):\n",
    "        return X[np.where(label == HAM)]\n",
    "    \n",
    "    def get_spam(self, X, label):\n",
    "        return X[np.where(label == SPAM)]\n",
    "                \n",
    "    def accuracy(self, result = None):\n",
    "        if np.size(result) > 1:\n",
    "            return np.mean(self.result == self.testing_label) # number of correct predictions / total testing cases\n",
    "        else:\n",
    "            print('there is no results!')\n",
    "            return 0\n",
    "\n",
    "    def naive_bayes(self, p_training = None):\n",
    "        if p_training != None:\n",
    "            # re-vectorize the data\n",
    "            self.vectorize(p_training)\n",
    "        # we use the multinomial naive bayes model from \n",
    "        # https://web.stanford.edu/class/cs124/lec/naivebayes.pdf\n",
    "        def get_prior():\n",
    "            '''get the prior of for the Naive Bayes method which will be\n",
    "            [fraction of ham emails, fraction of spam emails]'''\n",
    "            prior = self.N_TRAINING / self.N_TRAINING.sum()\n",
    "            return prior\n",
    "\n",
    "        def get_conditionals():\n",
    "            # split the traning data by label\n",
    "            training_ham = self.training_X[:self.N_TRAINING[HAM]]\n",
    "            training_spam = self.training_X[-self.N_TRAINING[SPAM]:]\n",
    "\n",
    "            # conditionals with Laplace smoothing\n",
    "            con_ham = (training_ham.sum(axis = 0) + 1) / (self.nwords(training_ham) + self.nvocab)\n",
    "            con_spam = (training_spam.sum(axis = 0) + 1) / (self.nwords(training_spam) + self.nvocab)\n",
    "            conditionals = np.asarray([con_ham, con_spam])\n",
    "            return conditionals\n",
    "\n",
    "        print('cross validating...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        prior = get_prior()\n",
    "        conditionals = get_conditionals()\n",
    "        # start applying labels to our testing data!\n",
    "        self.result = np.empty(self.N_TESTING.sum()) # the results of our classifier\n",
    "        for i in np.arange(self.N_TESTING.sum()):\n",
    "            # use log likelihood for easier calculation\n",
    "            loglike_ham = np.dot(np.log(conditionals[HAM]), self.testing_X[i]) + np.log(prior[HAM])\n",
    "            loglike_spam = np.dot(np.log(conditionals[SPAM]), self.testing_X[i]) + np.log(prior[SPAM])\n",
    "            self.result[i] = HAM if loglike_ham > loglike_spam else SPAM\n",
    "        print('testing took %.2f s' % (time.time() - start_time))\n",
    "        return self.result\n",
    "\n",
    "    def nearest_neighbor(self, p_training = None):\n",
    "        if p_training != None:\n",
    "            # re-vectorize the data\n",
    "            self.vectorize(p_training)\n",
    "\n",
    "        print('running classifier...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        def calculate_l1_distance(train_row, test_row):\n",
    "            diff_row = np.subtract(train_row, test_row)                           # find element wise difference\n",
    "            diff_row = np.absolute(diff_row)                                      # take absolute value of differences\n",
    "            distance = np.sum(diff_row)                                           # sum the distances\n",
    "            return distance\n",
    "\n",
    "\n",
    "        def calculate_l2_distance(train_row, test_row):\n",
    "            diff_row = np.subtract(train_row, test_row)\n",
    "            diff_row = np.square(diff_row)\n",
    "            distance = np.sum(diff_row)\n",
    "            return np.sqrt(distance)\n",
    "\n",
    "\n",
    "        def calculate_linf_distance(train_row, test_row):\n",
    "            diff_row = np.subtract(train_row, test_row)\n",
    "            diff_row = np.absolute(diff_row)\n",
    "            return np.amax(diff_row)\n",
    "\n",
    "        predicted_label_l1 = np.empty(shape = (len(self.testing_X), 1), dtype = int)\n",
    "        predicted_label_l2 = np.empty(shape = (len(self.testing_X), 1), dtype = int)\n",
    "        predicted_label_linf = np.empty(shape = (len(self.testing_X), 1), dtype = int)\n",
    "        for test_row, i in zip(self.testing_X, range(len(self.testing_X))):\n",
    "            row_distance_l1 = np.empty(shape = (len(self.training_X), 1), dtype = int)\n",
    "            row_distance_l2 = np.empty(shape = (len(self.training_X), 1), dtype = int)\n",
    "            row_distance_linf = np.empty(shape = (len(self.training_X), 1), dtype = int)\n",
    "            for train_row, j in zip(self.training_X, range(len(self.training_X))):\n",
    "                distance_l1 = calculate_l1_distance(train_row, test_row)\n",
    "                distance_l2 = calculate_l2_distance(train_row, test_row)\n",
    "                distance_linf = calculate_linf_distance(train_row, test_row)\n",
    "                row_distance_l1[j] = distance_l1                              # array of distances for each test row\n",
    "                row_distance_l2[j] = distance_l2\n",
    "                row_distance_linf[j] = distance_linf\n",
    "                # print(\"test row:\", test_row, \"  | label: \", self.testing_X_label[i])\n",
    "                # print(\"train row:\", train_row, \" | label: \", self.training_label[j])\n",
    "                # print(\"dist sum: \", distance)\n",
    "            min_dist_index_l1 = np.argmin(row_distance_l1)                    # min distance's index in array of distances\n",
    "            min_dist_index_l2 = np.argmin(row_distance_l2)\n",
    "            min_dist_index_linf = np.argmin(row_distance_linf)\n",
    "\n",
    "            predicted_label_l1[i] = self.training_label[min_dist_index_l1]\n",
    "            predicted_label_l2[i] = self.training_label[min_dist_index_l2]\n",
    "            predicted_label_linf[i] = self.training_label[min_dist_index_linf]\n",
    "            # print(\"-----------------------\")\n",
    "            # print(\"min dist: \", np.amin(row_distance))\n",
    "            # print(\"index of min: \", np.argmin(row_distance))\n",
    "            # print(\"predicted label: \", predicted_label[i])\n",
    "            # print(\"-----------------------\\n\")\n",
    "            self.result = [predicted_label_l1, predicted_label_l2, predicted_label_linf]\n",
    "        print('testing took %.2f s' % (time.time() - start_time))\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Classifier(HAM_LIST, SPAM_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing the emails...\n",
      "vectorizing took 4.02 s\n"
     ]
    }
   ],
   "source": [
    "# train_arr, train_arr_label, test_arr, test_arr_label = test.vectorize(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing the emails...\n",
      "vectorizing took 4.49 s\n",
      "cross validating...\n",
      "testing took 0.14 s\n"
     ]
    }
   ],
   "source": [
    "result = test.naive_bayes(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing the emails...\n",
      "vectorizing took 4.74 s\n",
      "running classifier...\n",
      "testing took 91.08 s\n"
     ]
    }
   ],
   "source": [
    "result = test.nearest_neighbor(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6364917040592716"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.accuracy(result[1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.N_TESTING.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6555"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.nvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
